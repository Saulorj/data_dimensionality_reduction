{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensionality Reduction\n",
    "Comparar os 4 métodos de redução de dimensões (PCA, SVD, NMF e Autoencoder) em relação ao modelo sem nenhum tipo de redução.  \n",
    "Para tornar o exercício mais interessante, será avaliado não só os 4 algoritmos de redução de dimensão, mas também vários modelos de classificação.  \n",
    "O objetivo final é termos a melhor opção entre o modelo de classificação e de redução (se for o caso).  \n",
    "\n",
    "### Base de dados - SK-Learn Breast Cancer\n",
    "\n",
    "Para o exercício será utilizado o dataset do sklearn **breast cancer** (UCI ML Breast Cancer) no que contém informações sobre câncer de mama e a classificação se é benigno ou maligno.  \n",
    "Maiores detalhes sobre o dataset disponível em  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\n",
    "\n",
    "***Fonte original do dataset***  \n",
    "The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is downloaded from:  \n",
    "https://goo.gl/U2Uwz2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o Dataset\n",
    "O conjunto de dados utilizado contém possui em seu target uma classificação binária do câncer em 'malignant', 'benign' (maligno ou benigno).  \n",
    "O objetivo, é criar um classificador que ao avaliar suas 30 features possa determinar o tipo do cancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando o conjunto de dados\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibindo as Features e targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: ['malignant' 'benign'] \n",
      "\n",
      "Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Target:', data.target_names, '\\n')\n",
    "print('Features:',data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as demais bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection \n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando os dados em treino e teste\n",
    "\n",
    "Para o exemplo utilizaremos uma base de tete de 40% do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "#Carregndo novamente já seprando os dados\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "#separando em treino e teste com 40% para o teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo os modelos que serão utilizados \n",
    "\n",
    "Em princípio faremos testes com 5 tipos declassificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado da avaiação\n",
      "  KNN: media 0.903112 (dp 0.029045 - max 0.942029)\n",
      "  DTC: media 0.894288 (dp 0.028798 - max 0.942029)\n",
      "  GB: media 0.935422 (dp 0.015174 - max 0.956522)\n",
      "  SVM: media 0.888491 (dp 0.044354 - max 0.955882)\n",
      "  RF: media 0.958909 (dp 0.011097 - max 0.971014)\n",
      "\n",
      "Melhor modelo: RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#import warnings\n",
    "#from sklearn.exceptions import ConvergenceWarning\n",
    "#warnings.filterwarnings(\"ignore\",  category = ConvergenceWarning)\n",
    "\n",
    "def rodar_modelo(models):\n",
    "    '''Executa os modelos definidos (parametros default) e retorna o modelo com melhor performance'''\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'accuracy'\n",
    "    print('Resultado da avaiação')\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results.mean())\n",
    "        names.append(name)\n",
    "        msg = \"  %s: media %f (dp %f - max %f)\" % (name, cv_results.mean(), cv_results.std(), cv_results.max())\n",
    "        print(msg)\n",
    "    pos_melhor_modelo = results.index(max(results))\n",
    "    return models[pos_melhor_modelo]\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "models.append(('GB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "melhor_modelo = rodar_modelo(models)\n",
    "print('')\n",
    "print('Melhor modelo:', melhor_modelo[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprimorando o modelo selecionado 'Random Forest Classifier'\n",
    "\n",
    "Utilizar o GridSearch para testar um conjunto de hipoteses de parametros. O objetivo é descobrir a melhor combinação entre eles e maximizar o resultado da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid={\n",
    "            'random_state': [None, 0, 1, 2, 3, 42],\n",
    "            'max_depth': [None, 1, 2, 3, 4, 15, 20],\n",
    "            'n_estimators': [5, 10, 100, 150, 200], \n",
    "            'max_features': ['auto', 'sqrt', 'log2', 1, 2, 3],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'min_samples_split': [2, 4, 6],\n",
    "            'min_samples_leaf': [3,4,5]\n",
    "        },\n",
    "        cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "\n",
    "grid_result = gsc.fit(X_train, y_train)\n",
    "best_params = grid_result.best_params_\n",
    "classifier = RandomForestClassifier(max_depth=best_params[\"max_depth\"], random_state=best_param[\"random_state\"],n_estimators = best_params[\"n_estimators\"], max_features = best_param[\"max_features\"],criterion=best_params[\"criterion\"], min_samples_split=best_params[\"min_samples_split\"], min_samples_leaf = best_params[\"min_samples_leaf\"])\n",
    "\n",
    "model = classifier.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) # avaliando o melhor estimador\n",
    "\n",
    "print(\"RandomForestClassifier GIRD\")\n",
    "for k, v in best_params.items():\n",
    "    print(' ', k, ': ',  v, '\\n')\n",
    "\n",
    "classification_report(y_test, y_pred, target_names=['Maligno', 'Benigno'])\n",
    "plot_confusion_matrix(classifier, X_test, y_test, cmap=\"Blues\", display_labels=['Maligno', 'Benigno'])  \n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando as features do modelo quanto a sua importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(10, 8), dpi=80)\n",
    "\n",
    "def avaliar_features(melhor_modelo):\n",
    "    model = melhor_modelo[1]\n",
    "    model.fit(X_train, y_train)\n",
    "    importance = model.feature_importances_\n",
    "    colunas = data.feature_names\n",
    "\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importance):\n",
    "        print('%s: %.5f' % (colunas[i],v))\n",
    "\n",
    "    # plot feature importance\n",
    "    plt.bar([x for x in range(len(importance))], importance)\n",
    "    plt.show()\n",
    "\n",
    "avaliar_features(melhor_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando os algoritmos de redução de dimensões para comparar os resultados\n",
    "\n",
    "Com base no gráfico, utilizaremos o número de features que possuem uma relevancia acima de 40% (arbitrário) mas estas não serão escolhidas manualmente, apenas a recomendação do número de dimensões que no caso 10 features (número de features que possuem +40% de relevancia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar a redução de dimensão em X e rodar o processo novamente com os melhores parametros\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "pca = sklearnPCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "#Aplicar o novo X no modelo já treinado\n",
    "model = classifier.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) # avaliando o melhor estimador\n",
    "classification_report(y_test, y_pred, target_names=['Maligno', 'Benigno'])\n",
    "plot_confusion_matrix(classifier, X_test, y_test, cmap=\"Blues\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD - Single Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar a redução de dimensão em X e rodar o processo novamente com os melhores parametros\n",
    "from sklearn.decomposition import SVD as sklearnSVD\n",
    "pca = sklearnPCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "#Aplicar o novo X no modelo já treinado\n",
    "model = classifier.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) # avaliando o melhor estimador\n",
    "classification_report(y_test, y_pred, target_names=['Maligno', 'Benigno'])\n",
    "plot_confusion_matrix(classifier, X_test, y_test, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF - Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF as sklearnNMF\n",
    "pca = sklearnPCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "#Aplicar o novo X no modelo já treinado\n",
    "model = classifier.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) # avaliando o melhor estimador\n",
    "classification_report(y_test, y_pred, target_names=['Maligno', 'Benigno'])\n",
    "plot_confusion_matrix(classifier, X_test, y_test, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "43c9e04d73dc828ac359c52be8edd92be05bfdb600886e1613060c70e57c8cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
